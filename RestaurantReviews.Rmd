---
title: "Restaurant Reviews"
output: html_document
date: "2024-12-17"
---

## Project Overview

This analysis helps restaurants evaluate customer feedback by examining reviews that focus on key aspects such as **ambience**, **staff behavior**, **food quality**, and **overall service**. Using text mining and sentiment analysis techniques, the code identifies common themes and sentiment trends, highlighting strengths and areas where improvements are needed.

Machine learning models are employed to predict whether a review is positive or negative based solely on the text content. This enables the restaurant to analyze sentiment even when star ratings are missing. Conversely, by combining star ratings with review text, the analysis can uncover specific reasons behind negative reviews, helping restaurants understand what factors—like slow service, poor ambience or unfriendly staff—are driving dissatisfaction.

Ultimately, this approach supports data-driven decisions to enhance the dining experience and increase customer satisfaction.

---

```{r}
# Load all necessary libraries for text mining, visualization, modeling, etc.
library(tidyverse)        # Data manipulation & visualization
library(tidytext)         # Text mining (tokenization, stop words, etc.)
library(tm)               # Text mining infrastructure
library(SnowballC)        # Stemming
library(textstem)         # Lemmatization
library(stringdist)       # String operations
library(cld2)             # Language detection
library(wordcloud2)       # Word cloud visualization
library(stopwords)        # Custom stopword list
library(syuzhet)          # Sentiment analysis (AFINN, NRC, etc.)
library(topicmodels)      # LDA topic modeling
library(caret)            # ML preprocessing and modeling
library(e1071)            # Naive Bayes
library(randomForest)     # for comparison
library(topicmodels).     # LDA topic modeling
library(dplyr)            # Data manipulation 
library(caret)            # ML training, preprocessing, evaluation
library(smotefamily)      # SMOTE oversampling to balance classes
```

```{r}
# Define the full path to your CSV file
file_path <- "/Users/arshia/Downloads/New_Delhi_reviews.csv"

# Load the CSV file
data <- read.csv(file_path, stringsAsFactors = FALSE)
```

```{r}
# View sample and structure
head(data)
str(data)
summary(data)
```

```{r}
# Clean data: Remove duplicates and NAs
data_clean <- data %>%
  distinct() %>%
  drop_na()

cat("Removed duplicates and NA rows:", nrow(data) - nrow(data_clean), "\n")
```

```{r}
# Optional: sample data for faster testing
set.seed(123)
data_clean <- sample_n(data_clean, min(5000, nrow(data_clean)))
```

```{r}
# Tokenize and remove stopwords
tidy_text <- data_clean %>%
  unnest_tokens(word, review_full) %>%
  anti_join(get_stopwords(), by = "word")

head(tidy_text)
```

```{r}
# Aggregate tokens back by rating_review
cleaned_data <- tidy_text %>%
  group_by(rating_review) %>%
  summarise(cleaned_text = paste(word, collapse = " "), .groups = "drop")

head(cleaned_data)
```

```{r}
# Further clean text: lowercase, remove punctuation, digits, non-ASCII
cleaned_data$cleaned_text <- cleaned_data$cleaned_text %>%
  tolower() %>%
  gsub("[[:punct:]]", " ", .) %>%
  gsub("[0-9]", " ", .) %>%
  iconv(from = "UTF-8", to = "ASCII//TRANSLIT", sub = "")
  
head(cleaned_data)
```

```{r}
# Apply stemming
cleaned_data$stemmed_text <- sapply(cleaned_data$cleaned_text, function(x) {
  words <- unlist(strsplit(x, "\\s+"))
  paste(wordStem(words, language = "en"), collapse = " ")
})
```

```{r}
# Apply lemmatization
cleaned_data$lemmatized_text <- sapply(cleaned_data$cleaned_text, lemmatize_strings)
```

```{r}
# Word frequency (for entire data)
word_freq <- tidy_text %>%
  count(word, sort = TRUE)

print(head(word_freq, 10))
```

```{r}
# Wordcloud visualization
wordcloud2(word_freq, size = 0.5, color = "blue")
```

```{r}
# Define a unique color for each star rating (1 to 5)
colors <- c("red", "maroon", "blue", "lightgreen", "darkgreen")

# Loop through each star rating from 1 to 5
for (rating in 1:5) {
  
  # Step 1: Filter reviews with the current star rating
  # Step 2: Tokenize the text into individual words
  # Step 3: Remove common stopwords (like "the", "and", etc.)
  # Step 4: Count word frequencies and get the top 10
  word_freq <- data_clean %>%
    filter(rating_review == rating) %>%
    unnest_tokens(word, review_full) %>%
    anti_join(get_stopwords(), by = "word") %>%
    count(word, sort = TRUE) %>%
    slice_head(n = 10)  # Select top 10 most frequent words
  
  # Create a bar plot for the top 10 words
  p <- ggplot(word_freq, aes(x = reorder(word, n), y = n)) +
    geom_bar(stat = "identity", fill = colors[rating]) +   # Use unique color
    coord_flip() +  # Flip axes for better readability
    labs(
      title = paste("Top 10 Most Frequent Words in", rating, "Star Reviews"),
      x = "Word",
      y = "Frequency"
    ) +
    theme_minimal()  # Clean theme
  
  # Display the plot
  print(p)
}
```

```{r}
# Categorize reviews by sentiment: negative (-1), neutral (0), positive (1)
categorize_review <- function(rating) {
  if (rating %in% c(0,1)) return(-1)
  else if (rating %in% c(4,5)) return(1)
  else return(0)
}

data_clean$review_category <- sapply(data_clean$rating_review, categorize_review)
table(data_clean$review_category)

ggplot(data_clean, aes(x = factor(review_category), fill = factor(review_category))) +
  geom_bar() +
  scale_x_discrete(labels = c("-1" = "Negative", "0" = "Neutral", "1" = "Positive")) +
  scale_fill_manual(
    values = c("-1" = "red", "0" = "blue", "1" = "green")  # Set custom colors
  ) +
  labs(title = "Review Category Distribution", x = "Review Category", y = "Count") +
  theme_minimal() +
  theme(legend.position = "none")  
```

```{r}
# Sentiment Analysis with NRC lexicon
tidy_reviews <- data_clean %>%
  unnest_tokens(word, review_full) %>%
  anti_join(get_stopwords(), by = "word")

sentiment_lexicon <- get_sentiments("nrc")

sentiment_words <- tidy_reviews %>%
  inner_join(sentiment_lexicon, by = "word") %>%
  filter(sentiment %in% c("positive", "negative"))

sentiment_counts <- sentiment_words %>%
  count(sentiment, word, sort = TRUE)

top_positive_words <- sentiment_counts %>% filter(sentiment == "positive") %>% head(10)
top_negative_words <- sentiment_counts %>% filter(sentiment == "negative") %>% head(10)

print(top_positive_words)
print(top_negative_words)

ggplot(top_positive_words, aes(x = reorder(word, n), y = n)) +
  geom_bar(stat = "identity", fill = "green") +
  coord_flip() +
  labs(title = "Top 10 Most Common Positive Words", x = "Word", y = "Frequency") +
  theme_minimal()

ggplot(top_negative_words, aes(x = reorder(word, n), y = n)) +
  geom_bar(stat = "identity", fill = "red") +
  coord_flip() +
  labs(title = "Top 10 Most Common Negative Words", x = "Word", y = "Frequency") +
  theme_minimal()
```

```{r}
# Sentiment scores with AFINN
sentiment_scores <- get_sentiment(data_clean$review_full, method = "afinn")
data_clean$sentiment_score <- sentiment_scores

ggplot(data_clean, aes(x = sentiment_score)) +
  geom_histogram(binwidth = 0.5, fill = "blue", color = "black", alpha = 0.7) +
  labs(title = "Sentiment Distribution of Reviews", x = "Sentiment Score", y = "Frequency")
```

```{r}
# Custom stop words to add
custom_stop_words <- data.frame(
  word = c("i", "we", "all", "this", "that", "a", "the", "and", "was", "of"),
  lexicon = "custom"
)

# Combine built-in stop words with custom ones
all_stop_words <- bind_rows(stop_words, custom_stop_words)

# Tokenize, remove stop words, count word frequencies
dtm <- data_clean %>%
  unnest_tokens(word, review_full) %>%
  anti_join(all_stop_words, by = "word") %>%
  count(rating_review, word) %>%
  cast_dtm(rating_review, word, n)
```

```{r}
# Run LDA with 5 topics
lda_model <- LDA(dtm, k = 5, control = list(seed = 123))

# Extract top terms per topic
lda_terms <- terms(lda_model, 10)
print(lda_terms)

# Tidy topic assignments
topic_assignments <- tidy(lda_model, matrix = "gamma")

# Plot topic distribution
ggplot(topic_assignments, aes(x = factor(topic), y = gamma)) +
  geom_boxplot() +
  labs(title = "Topic Distribution Across Reviews",
       x = "Topic", y = "Probability")
```

```{r}
# Extract bigrams from review text
bigrams <- data_clean %>%
  unnest_tokens(bigram, review_full, token = "ngrams", n = 2)

# Separate bigrams into two words
bigrams_separated <- bigrams %>%
  separate(bigram, into = c("word1", "word2"), sep = " ")

# Remove stop words from both words
data("stop_words")

custom_stop_words <- data.frame(
  word = c("i", "we", "all", "this", "that", "a", "the", "and", "was", "of"),
  lexicon = "custom"
)

all_stop_words <- bind_rows(stop_words, custom_stop_words)

bigrams_filtered <- bigrams_separated %>%
  filter(!word1 %in% all_stop_words$word,
         !word2 %in% all_stop_words$word)

# Reunite words into filtered bigrams
bigrams_filtered <- bigrams_filtered %>%
  unite(bigram, word1, word2, sep = " ")

# Count bigrams by rating
bigrams_with_ratings <- bigrams_filtered %>%
  count(rating_review, bigram, sort = TRUE)

# Define relevant keywords
relevant_words <- c("food", "service", "staff", "taste", "wait", "clean", 
                    "good", "bad", "best", "love", "quality", "slow", "delicious")

# Filter bigrams that contain relevant words
filtered_bigrams <- bigrams_with_ratings %>%
  filter(str_detect(bigram, paste(relevant_words, collapse = "|")))

# Define function to print and plot top bigrams for each star rating
plot_top_bigrams <- function(rating, color) {
  top_bigrams <- filtered_bigrams %>%
    filter(rating_review == rating) %>%
    slice_max(n, n = 10)

  # Print in console
  cat("\nTop 10 bigrams for", rating, "star reviews:\n")
  print(top_bigrams)

  # Plot
  ggplot(top_bigrams, aes(x = reorder(bigram, n), y = n)) +
    geom_bar(stat = "identity", fill = color, width = 0.7) +
    coord_flip() +
    labs(title = paste("Top 10 Most Frequent Bigrams in", rating, "Star Reviews"),
         x = "Bigram", y = "Frequency") +
    theme_minimal(base_size = 14)
}

# Step 9: Run for ratings 1 to 5
plot_top_bigrams(1, "red")
plot_top_bigrams(2, "maroon")
plot_top_bigrams(3, "blue")
plot_top_bigrams(4, "lightgreen")
plot_top_bigrams(5, "darkgreen")
```

```{r}
# Prepare data with binary sentiment label
data_clean$review_category <- sapply(data_clean$rating_review, function(r) {
  if (r %in% c(4, 5)) return("positive")
  else if (r %in% c(1, 2)) return("negative")
  else return(NA)  # We remove neutral 3-star reviews for classification
})

# Remove neutral reviews (NA labels)
data_nb <- data_clean %>% filter(!is.na(review_category))

# Split into train and test (80-20)
set.seed(123)
train_index <- createDataPartition(data_nb$review_category, p = 0.8, list = FALSE)
train_data <- data_nb[train_index, ]
test_data <- data_nb[-train_index, ]

# Create a corpus and Document-Term Matrix with TF-IDF

# Combine train and test for consistent DTM vocab
all_text <- c(train_data$review_full, test_data$review_full)
corpus <- VCorpus(VectorSource(all_text))

# Preprocess corpus: lowercase, remove punctuation, numbers, stopwords
corpus_clean <- tm_map(corpus, content_transformer(tolower))
corpus_clean <- tm_map(corpus_clean, removePunctuation)
corpus_clean <- tm_map(corpus_clean, removeNumbers)
corpus_clean <- tm_map(corpus_clean, removeWords, c(stopwords("en"), "i", "we", "all", "this", "that", "a", "the", "and", "was", "of"))
corpus_clean <- tm_map(corpus_clean, stripWhitespace)

# Create TF-IDF weighted DTM
dtm_tfidf <- DocumentTermMatrix(corpus_clean, control = list(weighting = weightTfIdf))

# Split back into train and test DTM
train_dtm <- dtm_tfidf[1:nrow(train_data), ]
test_dtm <- dtm_tfidf[(nrow(train_data) + 1):nrow(dtm_tfidf), ]

# Remove sparse terms (optional for performance)
train_dtm <- removeSparseTerms(train_dtm, 0.99)
test_dtm <- test_dtm[, colnames(train_dtm)]  # Keep only train columns in test

# Convert to matrix and then dataframe for modeling
train_matrix <- as.data.frame(as.matrix(train_dtm))
test_matrix <- as.data.frame(as.matrix(test_dtm))

# Add label
train_matrix$sentiment <- factor(train_data$review_category)
test_matrix$sentiment <- factor(test_data$review_category)

# Train Naive Bayes classifier
model_nb <- naiveBayes(sentiment ~ ., data = train_matrix)

# Predict on test set
predictions <- predict(model_nb, test_matrix)

# Evaluate
conf_mat <- confusionMatrix(predictions, test_matrix$sentiment)
print(conf_mat)
```

```{r}
# Use only positive and negative reviews
data_bal <- data_clean %>%
  filter(review_category %in% c("positive", "negative")) %>%
  select(review_full, review_category)

# Create TF-IDF features
corpus <- VCorpus(VectorSource(data_bal$review_full))
corpus <- tm_map(corpus, content_transformer(tolower))
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, removeNumbers)
corpus <- tm_map(corpus, removeWords, c(stopwords("en"), "i", "we", "all", "this", "that", "a", "the", "and", "was", "of"))
corpus <- tm_map(corpus, stripWhitespace)

dtm <- DocumentTermMatrix(corpus, control = list(weighting = weightTfIdf))
dtm <- removeSparseTerms(dtm, 0.99)

# Convert to data frame
dtm_df <- as.data.frame(as.matrix(dtm))
dtm_df$review_category <- as.factor(data_bal$review_category)

# Prepare for SMOTE
dtm_df$label <- ifelse(dtm_df$review_category == "positive", 1, 0)
dtm_df$review_category <- NULL  # remove original factor label

X <- dtm_df[, !(names(dtm_df) %in% "label")]
y <- dtm_df$label

# Apply balanced SMOTE
set.seed(123)
smote_output <- SMOTE(X, y, K = 5, dup_size = 0)
data_smote <- smote_output$data
colnames(data_smote)[ncol(data_smote)] <- "label"
data_smote$label <- factor(ifelse(data_smote$label == 1, "positive", "negative"))

# Train/Test Split
set.seed(123)
train_index <- createDataPartition(data_smote$label, p = 0.8, list = FALSE)
train_data <- data_smote[train_index, ]
test_data <- data_smote[-train_index, ]

# Train Naive Bayes Model
model_nb <- naiveBayes(label ~ ., data = train_data)

# Predict
pred_nb <- predict(model_nb, test_data)

# Evaluate
confusionMatrix(pred_nb, test_data$label)
```

```{r}
# Use only positive and negative reviews
data_bal <- data_clean %>%
  filter(review_category %in% c("positive", "negative")) %>%
  select(review_full, review_category)

# Create TF-IDF features
corpus <- VCorpus(VectorSource(data_bal$review_full))
corpus <- tm_map(corpus, content_transformer(tolower))
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, removeNumbers)
corpus <- tm_map(corpus, removeWords, c(stopwords("en"), "i", "we", "all", "this", "that", "a", "the", "and", "was", "of"))
corpus <- tm_map(corpus, stripWhitespace)

dtm <- DocumentTermMatrix(corpus, control = list(weighting = weightTfIdf))
dtm <- removeSparseTerms(dtm, 0.99)

# Convert to data frame and clean column names
dtm_df <- as.data.frame(as.matrix(dtm))
colnames(dtm_df) <- make.names(colnames(dtm_df), unique = TRUE)  # ✅ FIXED HERE
dtm_df$review_category <- as.factor(data_bal$review_category)

# Prepare for SMOTE
dtm_df$label <- ifelse(dtm_df$review_category == "positive", 1, 0)
dtm_df$review_category <- NULL  # remove original factor label

X <- dtm_df[, !(names(dtm_df) %in% "label")]
y <- dtm_df$label

# Apply balanced SMOTE
set.seed(123)
smote_output <- SMOTE(X, y, K = 5, dup_size = 0)
data_smote <- smote_output$data
colnames(data_smote)[ncol(data_smote)] <- "label"
data_smote$label <- factor(ifelse(data_smote$label == 1, "positive", "negative"))

# Train/Test Split
set.seed(123)
train_index <- createDataPartition(data_smote$label, p = 0.8, list = FALSE)
train_data <- data_smote[train_index, ]
test_data <- data_smote[-train_index, ]

# Clean column names again before modeling (precautionary)
colnames(train_data) <- make.names(colnames(train_data), unique = TRUE)
colnames(test_data) <- make.names(colnames(test_data), unique = TRUE)

# Train Random Forest Model
library(randomForest)
set.seed(123)
model_rf <- randomForest(label ~ ., data = train_data, ntree = 100, importance = TRUE)

# Predict
pred_rf <- predict(model_rf, test_data)

# Evaluate
conf_mat_rf <- confusionMatrix(pred_rf, test_data$label)
print(conf_mat_rf)
```

---

##Interpretation

The Random Forest + SMOTE combination outperforms the others by a large margin — both in accuracy and in how well it handles class imbalance (high Kappa, Balanced Accuracy).

The original Naive Bayes model (without SMOTE) severely underperforms due to class imbalance, which is typical in real-world data.

SMOTE improves all models by creating a more balanced dataset, helping classifiers learn better decision boundaries.

## Summary

This project analyzed restaurant reviews from New Delhi using text mining and sentiment analysis techniques to extract meaningful insights. Key steps included:

- **Data Cleaning & Preprocessing:** Removed duplicates, handled missing data, tokenized text, removed stopwords and applied stemming and lemmatization for better text normalization.
- **Exploratory Analysis:** Identified frequent words and bigrams by star rating, visualized sentiment distributions and examined common positive and negative terms.
- **Sentiment Categorization:** Classified reviews into negative, neutral and positive categories based on star ratings to facilitate sentiment-focused analysis.
- **Topic Modeling:** Applied Latent Dirichlet Allocation (LDA) to uncover major themes present across the reviews.
- **Machine Learning Models:** Built Naive Bayes and Random Forest classifiers to predict sentiment labels from review text using TF-IDF features.
- **Class Imbalance Handling:** Employed SMOTE oversampling to balance the dataset, significantly improving model performance.
- **Model Evaluation:** Random Forest with SMOTE showed the best accuracy and robustness indicating its suitability for sentiment classification on imbalanced review data.

Overall, this comprehensive approach enables restaurants to gain deep insights into customer feedback, identify strengths and weaknesses and make data-driven decisions to improve service quality and customer satisfaction.

---


